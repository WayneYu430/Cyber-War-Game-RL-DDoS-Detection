{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NASim - tutorial\n",
    "Network Attack Simulator (NASim) is a lightweight, high-level network attack simulator \n",
    "https://networkattacksimulator.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Starting NASim using OpenAI gym\n",
    "NASim could use OpenAI gym, as well as directly use itself's class\n",
    "\n",
    "All benchmark scenarios can be loaded using `gym.make()`\n",
    "\n",
    "- Custom scenarios must be loaded using the nasim library directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"nasim:Tiny-PO-v0\")\n",
    "# env.action_space\n",
    "# env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Starting a NASim Environment\n",
    "Interaction with NASim is done primarily via the NASimEnv class\n",
    "## Environment Settings\n",
    "1. scenario definition \n",
    "The scenario defines the network properties and the pen-tester specific information (e.g. exploits available, etc).\n",
    "2. Three optional args\n",
    "    1. `fully_obs`\n",
    "    - True: easy-mode, check environment\n",
    "    - False: real-setting, agent has no nowledge of the location, configuration and value of every host on the network and recieves only observations of features of the directly related to the action performed at each step.\n",
    "    2. `flat_actions`\n",
    "    3. `flat_obs`\n",
    "\n",
    "## Loading an Environment from a Scenario\n",
    "- Making an existing scenario\n",
    "- Loading a scenario from a YAML file\n",
    "\n",
    "# Interacting with NASim Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sample Agent from the tutorial...\n",
    "\n",
    "# importing sys\n",
    "import sys\n",
    "import nasim\n",
    "\n",
    "# sys.path.insert(0, 'F:/Anaconda3/envs/rl_env/Lib/site-packages/nasim/agents')\n",
    "from ql_agent import TabularQLearningAgent\n",
    "from dqn_agent import DQNAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN with config:\n",
      "{'batch_size': 32,\n",
      " 'env': <nasim.envs.environment.NASimEnv object at 0x00000248E7865580>,\n",
      " 'exploration_steps': 10000,\n",
      " 'final_epsilon': 0.05,\n",
      " 'gamma': 0.99,\n",
      " 'hidden_sizes': [64, 64],\n",
      " 'kwargs': {},\n",
      " 'lr': 0.001,\n",
      " 'replay_size': 10000,\n",
      " 'seed': None,\n",
      " 'self': <dqn_agent.DQNAgent object at 0x00000248E77DECD0>,\n",
      " 'target_update_freq': 1000,\n",
      " 'training_steps': 20000,\n",
      " 'verbose': True}\n",
      "\n",
      "Using Neural Network running on device=cpu:\n",
      "DQN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=56, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (out): Linear(in_features=64, out_features=18, bias=True)\n",
      ")\n",
      "\n",
      "Starting training\n",
      "\n",
      "Episode 10:\n",
      "\tsteps done = 999 / 20000\n",
      "\treturn = 81.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 20:\n",
      "\tsteps done = 1691 / 20000\n",
      "\treturn = 158.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 30:\n",
      "\tsteps done = 2801 / 20000\n",
      "\treturn = 58.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 40:\n",
      "\tsteps done = 3595 / 20000\n",
      "\treturn = 148.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 50:\n",
      "\tsteps done = 4394 / 20000\n",
      "\treturn = 50.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 60:\n",
      "\tsteps done = 5227 / 20000\n",
      "\treturn = 129.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 70:\n",
      "\tsteps done = 5802 / 20000\n",
      "\treturn = 130.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 80:\n",
      "\tsteps done = 6507 / 20000\n",
      "\treturn = 127.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 90:\n",
      "\tsteps done = 8128 / 20000\n",
      "\treturn = 144.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 100:\n",
      "\tsteps done = 9814 / 20000\n",
      "\treturn = -80.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 110:\n",
      "\tsteps done = 12169 / 20000\n",
      "\treturn = 168.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 120:\n",
      "\tsteps done = 12729 / 20000\n",
      "\treturn = 176.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 130:\n",
      "\tsteps done = 13306 / 20000\n",
      "\treturn = 188.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 140:\n",
      "\tsteps done = 13490 / 20000\n",
      "\treturn = 184.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 150:\n",
      "\tsteps done = 13639 / 20000\n",
      "\treturn = 184.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 160:\n",
      "\tsteps done = 13835 / 20000\n",
      "\treturn = 174.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 170:\n",
      "\tsteps done = 14405 / 20000\n",
      "\treturn = -12.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 180:\n",
      "\tsteps done = 18299 / 20000\n",
      "\treturn = 169.0\n",
      "\tgoal = True\n",
      "Training complete\n",
      "\n",
      "Episode 186:\n",
      "\tsteps done = 20000 / 20000\n",
      "\treturn = -457.0\n",
      "\tgoal = False\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = nasim.make_benchmark(\"tiny\")\n",
    "state_dim = env.observation_space # 状态维度\n",
    "action_dim = env.action_space # 动作维度\n",
    "agent = DQNAgent(env=env)\n",
    "\n",
    "agent.train()\n",
    "# gent.run_eval_episode(render='store_true')\n",
    "print(\"Done\")\n",
    "# print(\"Total reward =\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b9c7ed270f474c0b985bf49c8749526c0e6b1f84c1b1c6765e44e2938a6f9c8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}