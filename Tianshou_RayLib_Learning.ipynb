{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RayLib\n",
    "https://docs.ray.io/en/master/ray-overview/installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 21:38:57,893\tINFO worker.py:973 -- Calling ray.init() again after it has already been called.\n",
      "2022-06-22 21:38:58,085\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-06-22 21:38:58,086\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-06-22 21:38:58,087\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m pygame 2.1.0 (SDL 2.0.16, Python 3.8.13)\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m attack-ip: 192.168.1.106     pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m attack-ip: 192.168.1.172     pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m attack-ip: 192.168.1.137     pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m attack-ip: 192.168.1.176     pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m attack-ip: 192.168.1.83      pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m attack-ip: 192.168.1.60      pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m attack-ip: 192.168.1.10      pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m attack-ip: 192.168.1.36      pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m attack-ip: 192.168.1.187     pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m attack-ip: 192.168.1.110     pkt sending: 0\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m user  -ip: 192.168.1.123     pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m user  -ip: 192.168.1.81      pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m user  -ip: 192.168.1.102     pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m user  -ip: 192.168.1.214     pkt sending: 0; \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m user  -ip: 192.168.1.250     pkt sending: 0\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m server-ip: 192.168.1.127     pkt sending: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m   logger.warn(\n",
      "2022-06-22 21:39:34,477\tINFO trainable.py:159 -- Trainable.setup took 36.395 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-06-22 21:39:34,480\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-06-22 21:39:34,598\tWARNING trainer.py:1124 -- Worker crashed during call to `step_attempt()`. To try to continue training without failed worker(s), set `ignore_worker_failures=True`. To try to recover the failed worker(s), set `recreate_failed_workers=True`.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\gym\\spaces\\box.py:142: UserWarning: \u001B[33mWARN: Casting input x to numpy array.\u001B[0m\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=9188)\u001B[0m   logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    },
    {
     "ename": "RayTaskError(TypeError)",
     "evalue": "\u001B[36mray::RolloutWorker.sample()\u001B[39m (pid=9188, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023ED0577700>)\n  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 616, in ray._raylet.execute_task.function_executor\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 675, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 462, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 825, in sample\n    batches = [self.input_reader.next()]\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 115, in next\n    batches = [self.get_data()]\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 288, in get_data\n    item = next(self._env_runner)\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 671, in _env_runner\n    active_envs, to_eval, outputs = _process_observations(\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 898, in _process_observations\n    prep_obs = preprocessor.transform(raw_obs)\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 205, in transform\n    self.check_shape(observation)\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 68, in check_shape\n    if not self._obs_space.contains(observation):\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\gym\\spaces\\box.py\", line 143, in contains\n    x = np.asarray(x, dtype=self.dtype)\nTypeError: float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRayTaskError(TypeError)\u001B[0m                   Traceback (most recent call last)",
      "\u001B[1;32md:\\Wayne_OMEN_Onedrive\\OneDrive\\Telecommunications\\MSc Project\\code\\RayLib_Learning.ipynb Cell 2'\u001B[0m in \u001B[0;36m<cell line: 31>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000001?line=29'>30</a>\u001B[0m n_iter \u001B[39m=\u001B[39m \u001B[39m1\u001B[39m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000001?line=30'>31</a>\u001B[0m \u001B[39mfor\u001B[39;00m n \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(n_iter):\n\u001B[1;32m---> <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000001?line=31'>32</a>\u001B[0m     result \u001B[39m=\u001B[39m agent\u001B[39m.\u001B[39;49mtrain()\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000001?line=32'>33</a>\u001B[0m     \u001B[39m# chkpt_file = agent.save(chkpt_root)\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000001?line=33'>34</a>\u001B[0m     \u001B[39mprint\u001B[39m(status\u001B[39m.\u001B[39mformat(\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000001?line=34'>35</a>\u001B[0m             n \u001B[39m+\u001B[39m \u001B[39m1\u001B[39m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000001?line=35'>36</a>\u001B[0m             result[\u001B[39m\"\u001B[39m\u001B[39mepisode_reward_min\u001B[39m\u001B[39m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000001?line=38'>39</a>\u001B[0m             result[\u001B[39m\"\u001B[39m\u001B[39mepisode_len_mean\u001B[39m\u001B[39m\"\u001B[39m]\n\u001B[0;32m     <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000001?line=39'>40</a>\u001B[0m             ))\n",
      "File \u001B[1;32mf:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\tune\\trainable.py:360\u001B[0m, in \u001B[0;36mTrainable.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    358\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_warmup_time \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime() \u001B[39m-\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_start_time\n\u001B[0;32m    359\u001B[0m start \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n\u001B[1;32m--> 360\u001B[0m result \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mstep()\n\u001B[0;32m    361\u001B[0m \u001B[39massert\u001B[39;00m \u001B[39misinstance\u001B[39m(result, \u001B[39mdict\u001B[39m), \u001B[39m\"\u001B[39m\u001B[39mstep() needs to return a dict.\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[0;32m    363\u001B[0m \u001B[39m# We do not modify internal state nor update this result if duplicate.\u001B[39;00m\n",
      "File \u001B[1;32mf:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1131\u001B[0m, in \u001B[0;36mTrainer.step\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1122\u001B[0m     \u001B[39m# Error out.\u001B[39;00m\n\u001B[0;32m   1123\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[0;32m   1124\u001B[0m         logger\u001B[39m.\u001B[39mwarning(\n\u001B[0;32m   1125\u001B[0m             \u001B[39m\"\u001B[39m\u001B[39mWorker crashed during call to `step_attempt()`. \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[0;32m   1126\u001B[0m             \u001B[39m\"\u001B[39m\u001B[39mTo try to continue training without failed \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1129\u001B[0m             \u001B[39m\"\u001B[39m\u001B[39m`recreate_failed_workers=True`.\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[0;32m   1130\u001B[0m         )\n\u001B[1;32m-> 1131\u001B[0m         \u001B[39mraise\u001B[39;00m e\n\u001B[0;32m   1132\u001B[0m \u001B[39m# Any other exception.\u001B[39;00m\n\u001B[0;32m   1133\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mException\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[0;32m   1134\u001B[0m     \u001B[39m# Allow logs messages to propagate.\u001B[39;00m\n",
      "File \u001B[1;32mf:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1112\u001B[0m, in \u001B[0;36mTrainer.step\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1109\u001B[0m \u001B[39mwhile\u001B[39;00m \u001B[39mnot\u001B[39;00m step_ctx\u001B[39m.\u001B[39mshould_stop(step_attempt_results):\n\u001B[0;32m   1110\u001B[0m     \u001B[39m# Try to train one step.\u001B[39;00m\n\u001B[0;32m   1111\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m-> 1112\u001B[0m         step_attempt_results \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mstep_attempt()\n\u001B[0;32m   1113\u001B[0m     \u001B[39m# @ray.remote RolloutWorker failure.\u001B[39;00m\n\u001B[0;32m   1114\u001B[0m     \u001B[39mexcept\u001B[39;00m RayError \u001B[39mas\u001B[39;00m e:\n\u001B[0;32m   1115\u001B[0m         \u001B[39m# Try to recover w/o the failed worker.\u001B[39;00m\n",
      "File \u001B[1;32mf:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:1214\u001B[0m, in \u001B[0;36mTrainer.step_attempt\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1212\u001B[0m \u001B[39m# No evaluation necessary, just run the next training iteration.\u001B[39;00m\n\u001B[0;32m   1213\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m evaluate_this_iter:\n\u001B[1;32m-> 1214\u001B[0m     step_results \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_exec_plan_or_training_iteration_fn()\n\u001B[0;32m   1215\u001B[0m \u001B[39m# We have to evaluate in this training iteration.\u001B[39;00m\n\u001B[0;32m   1216\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m   1217\u001B[0m     \u001B[39m# No parallelism.\u001B[39;00m\n\u001B[0;32m   1218\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mconfig[\u001B[39m\"\u001B[39m\u001B[39mevaluation_parallel_to_training\u001B[39m\u001B[39m\"\u001B[39m]:\n",
      "File \u001B[1;32mf:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py:2209\u001B[0m, in \u001B[0;36mTrainer._exec_plan_or_training_iteration_fn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2207\u001B[0m \u001B[39mwith\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001B[0;32m   2208\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mconfig[\u001B[39m\"\u001B[39m\u001B[39m_disable_execution_plan_api\u001B[39m\u001B[39m\"\u001B[39m]:\n\u001B[1;32m-> 2209\u001B[0m         results \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtraining_iteration()\n\u001B[0;32m   2210\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[0;32m   2211\u001B[0m         results \u001B[39m=\u001B[39m \u001B[39mnext\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mtrain_exec_impl)\n",
      "File \u001B[1;32mf:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\agents\\ppo\\ppo.py:437\u001B[0m, in \u001B[0;36mPPOTrainer.training_iteration\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    433\u001B[0m     train_batch \u001B[39m=\u001B[39m synchronous_parallel_sample(\n\u001B[0;32m    434\u001B[0m         worker_set\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mworkers, max_agent_steps\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mconfig[\u001B[39m\"\u001B[39m\u001B[39mtrain_batch_size\u001B[39m\u001B[39m\"\u001B[39m]\n\u001B[0;32m    435\u001B[0m     )\n\u001B[0;32m    436\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m--> 437\u001B[0m     train_batch \u001B[39m=\u001B[39m synchronous_parallel_sample(\n\u001B[0;32m    438\u001B[0m         worker_set\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mworkers, max_env_steps\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mconfig[\u001B[39m\"\u001B[39;49m\u001B[39mtrain_batch_size\u001B[39;49m\u001B[39m\"\u001B[39;49m]\n\u001B[0;32m    439\u001B[0m     )\n\u001B[0;32m    440\u001B[0m train_batch \u001B[39m=\u001B[39m train_batch\u001B[39m.\u001B[39mas_multi_agent()\n\u001B[0;32m    441\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m train_batch\u001B[39m.\u001B[39magent_steps()\n",
      "File \u001B[1;32mf:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py:99\u001B[0m, in \u001B[0;36msynchronous_parallel_sample\u001B[1;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001B[0m\n\u001B[0;32m     96\u001B[0m     sample_batches \u001B[39m=\u001B[39m [worker_set\u001B[39m.\u001B[39mlocal_worker()\u001B[39m.\u001B[39msample()]\n\u001B[0;32m     97\u001B[0m \u001B[39m# Loop over remote workers' `sample()` method in parallel.\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m---> 99\u001B[0m     sample_batches \u001B[39m=\u001B[39m ray\u001B[39m.\u001B[39;49mget(\n\u001B[0;32m    100\u001B[0m         [worker\u001B[39m.\u001B[39;49msample\u001B[39m.\u001B[39;49mremote() \u001B[39mfor\u001B[39;49;00m worker \u001B[39min\u001B[39;49;00m worker_set\u001B[39m.\u001B[39;49mremote_workers()]\n\u001B[0;32m    101\u001B[0m     )\n\u001B[0;32m    102\u001B[0m \u001B[39m# Update our counters for the stopping criterion of the while loop.\u001B[39;00m\n\u001B[0;32m    103\u001B[0m \u001B[39mfor\u001B[39;00m b \u001B[39min\u001B[39;00m sample_batches:\n",
      "File \u001B[1;32mf:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001B[0m, in \u001B[0;36mclient_mode_hook.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    103\u001B[0m     \u001B[39mif\u001B[39;00m func\u001B[39m.\u001B[39m\u001B[39m__name__\u001B[39m \u001B[39m!=\u001B[39m \u001B[39m\"\u001B[39m\u001B[39minit\u001B[39m\u001B[39m\"\u001B[39m \u001B[39mor\u001B[39;00m is_client_mode_enabled_by_default:\n\u001B[0;32m    104\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mgetattr\u001B[39m(ray, func\u001B[39m.\u001B[39m\u001B[39m__name__\u001B[39m)(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n\u001B[1;32m--> 105\u001B[0m \u001B[39mreturn\u001B[39;00m func(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n",
      "File \u001B[1;32mf:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\worker.py:1831\u001B[0m, in \u001B[0;36mget\u001B[1;34m(object_refs, timeout)\u001B[0m\n\u001B[0;32m   1829\u001B[0m     worker\u001B[39m.\u001B[39mcore_worker\u001B[39m.\u001B[39mdump_object_store_memory_usage()\n\u001B[0;32m   1830\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(value, RayTaskError):\n\u001B[1;32m-> 1831\u001B[0m     \u001B[39mraise\u001B[39;00m value\u001B[39m.\u001B[39mas_instanceof_cause()\n\u001B[0;32m   1832\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m   1833\u001B[0m     \u001B[39mraise\u001B[39;00m value\n",
      "\u001B[1;31mRayTaskError(TypeError)\u001B[0m: \u001B[36mray::RolloutWorker.sample()\u001B[39m (pid=9188, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000023ED0577700>)\n  File \"python\\ray\\_raylet.pyx\", line 665, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 669, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 616, in ray._raylet.execute_task.function_executor\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 675, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 462, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 825, in sample\n    batches = [self.input_reader.next()]\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 115, in next\n    batches = [self.get_data()]\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 288, in get_data\n    item = next(self._env_runner)\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 671, in _env_runner\n    active_envs, to_eval, outputs = _process_observations(\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 898, in _process_observations\n    prep_obs = preprocessor.transform(raw_obs)\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 205, in transform\n    self.check_shape(observation)\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 68, in check_shape\n    if not self._obs_space.contains(observation):\n  File \"f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\gym\\spaces\\box.py\", line 143, in contains\n    x = np.asarray(x, dtype=self.dtype)\nTypeError: float() argument must be a string or a number, not 'dict'"
     ]
    }
   ],
   "source": [
    "import gym, ray\n",
    "import gym_cyberwargame\n",
    "from ray.rllib.agents import ppo,cql    \n",
    "# from gym_cyberwargame.envs import CyberWarGameEnv\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "select_env = \"CyberWarGame-v0\"\n",
    "register_env(select_env, lambda config: CyberWarGameEnv())\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "# config[\"framework\"] = \"torch\"\n",
    "config[\"num_workers\"] = 1\n",
    "config['disable_env_checking'] = True\n",
    "# print(pretty_print(config))\n",
    "agent = ppo.PPOTrainer(config, env=select_env)\n",
    "#ray.init()\n",
    "\n",
    "# trainer = ppo.PPOTrainer(env=CyberWarGameEnv, config={\n",
    "#     \"framework\": \"torch\",\n",
    "#     \"env_config\": {},  # config to pass to env class\n",
    "# })\n",
    "\n",
    "status = \"{:2d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:4.2f} saved {}\"\n",
    "n_iter = 1\n",
    "for n in range(n_iter):\n",
    "    result = agent.train()\n",
    "    # chkpt_file = agent.save(chkpt_root)\n",
    "    print(status.format(\n",
    "            n + 1,\n",
    "            result[\"episode_reward_min\"],\n",
    "            result[\"episode_reward_mean\"],\n",
    "            result[\"episode_reward_max\"],\n",
    "            result[\"episode_len_mean\"]\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ray.rllib.utils import check_env\n",
    "from gym_cyberwargame.envs import CyberWarGameEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "select_env = \"gym_cyberwargame/CyberWarGame-v0\"\n",
    "register_env(select_env, lambda config: CyberWarGameEnv())\n",
    "# Fail to check the reset() method\n",
    "check_env(CyberWarGameEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from elegantrl.agents import AgentModSAC, AgentPPO\n",
    "from elegantrl.train.config import get_gym_env_args, Arguments\n",
    "from elegantrl.train.run import *\n",
    "\n",
    "gym.logger.set_level(40)  # Block warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: env.action_space.high [1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'env_num': 1,\n",
       " 'env_name': 'BipedalWalker-v3',\n",
       " 'max_step': 1600,\n",
       " 'state_dim': 24,\n",
       " 'action_dim': 4,\n",
       " 'if_discrete': False,\n",
       " 'target_return': 300}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gym_env_args(gym.make('BipedalWalker-v3'), if_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_args = {\n",
    "    'env_num': 1,\n",
    "    'env_name': 'BipedalWalker-v3',\n",
    "    'max_step': 1600,\n",
    "    'state_dim': 24,\n",
    "    'action_dim': 4,\n",
    "    'if_discrete': False,\n",
    "    'target_return': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_func = gym.make\n",
    "env_args = {\n",
    "    'env_num': 1,\n",
    "    'env_name': 'BipedalWalker-v3',\n",
    "    'max_step': 1600,\n",
    "    'state_dim': 24,\n",
    "    'action_dim': 4,\n",
    "    'if_discrete': False,\n",
    "    'target_return': 300,\n",
    "    'id': 'BipedalWalker-v3',\n",
    "}\n",
    "\n",
    "args = Arguments(AgentPPO, env_func=env_func, env_args=env_args)\n",
    "\n",
    "args.target_step = args.max_step * 4\n",
    "args.gamma = 0.98\n",
    "args.eval_times = 2 ** 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Arguments Remove cwd: ./result/BipedalWalker-v3_PPO_0\n",
      "################################################################################\n",
      "ID     Step    Time |    maxR    curR    curS |    objC   objA   etc.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32md:\\Wayne_OMEN_Onedrive\\OneDrive\\Telecommunications\\MSc Project\\code\\RayLib_Learning.ipynb Cell 8'\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000007?line=0'>1</a>\u001B[0m args\u001B[39m.\u001B[39mlearner_gpus \u001B[39m=\u001B[39m \u001B[39m0\u001B[39m\n\u001B[0;32m      <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000007?line=1'>2</a>\u001B[0m args\u001B[39m.\u001B[39mif_use_per \u001B[39m=\u001B[39m \u001B[39mFalse\u001B[39;00m\n\u001B[1;32m----> <a href='vscode-notebook-cell:/d%3A/Wayne_OMEN_Onedrive/OneDrive/Telecommunications/MSc%20Project/code/RayLib_Learning.ipynb#ch0000007?line=2'>3</a>\u001B[0m train_and_evaluate(args)\n",
      "File \u001B[1;32mf:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\elegantrl\\train\\run.py:62\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m     59\u001B[0m buffer \u001B[39m=\u001B[39m init_buffer(args, gpu_id)\n\u001B[0;32m     60\u001B[0m evaluator \u001B[39m=\u001B[39m init_evaluator(args, gpu_id)\n\u001B[1;32m---> 62\u001B[0m agent\u001B[39m.\u001B[39mstate \u001B[39m=\u001B[39m env\u001B[39m.\u001B[39;49mreset()\n\u001B[0;32m     63\u001B[0m \u001B[39mif\u001B[39;00m args\u001B[39m.\u001B[39mif_off_policy:\n\u001B[0;32m     64\u001B[0m     trajectory, step \u001B[39m=\u001B[39m agent\u001B[39m.\u001B[39mexplore_env(env, args\u001B[39m.\u001B[39mnum_seed_steps \u001B[39m*\u001B[39m args\u001B[39m.\u001B[39mnum_steps_per_episode, \u001B[39mTrue\u001B[39;00m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "args.learner_gpus = 0\n",
    "args.if_use_per = False\n",
    "train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tianshou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym, torch, numpy as np, torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "\n",
    "\n",
    "task = 'CartPole-v0'\n",
    "lr, epoch, batch_size = 1e-3, 10, 64\n",
    "train_num, test_num = 10, 100\n",
    "gamma, n_step, target_freq = 0.9, 3, 320\n",
    "buffer_size = 20000\n",
    "eps_train, eps_test = 0.1, 0.05\n",
    "step_per_epoch, step_per_collect = 10000, 10\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))  # TensorBoard is supported!\n",
    "# For other loggers: https://tianshou.readthedocs.io/en/master/tutorials/logger.html\n",
    "\n",
    "# you can also try with SubprocVectorEnv\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: gym.make(task) for _ in range(train_num)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: gym.make(task) for _ in range(test_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State_shape (4,), Action_shape in 2\n"
     ]
    }
   ],
   "source": [
    "from tianshou.utils.net.common import Net\n",
    "# you can define other net by following the API:\n",
    "# https://tianshou.readthedocs.io/en/master/tutorials/dqn.html#build-the-network\n",
    "env = gym.make(task)\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[128, 128, 128])\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "print(\"State_shape {}, Action_shape in {}\".format(state_shape, action_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(net, optim, gamma, n_step, target_update_freq=target_freq)\n",
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [00:13, 769.25it/s, env_step=10000, len=135, loss=0.309, n/ep=0, n/st=10, rew=135.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 144.600000 ± 18.101381, best_reward: 144.600000 ± 18.101381 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10001it [00:16, 594.87it/s, env_step=20000, len=185, loss=0.328, n/ep=0, n/st=10, rew=185.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 165.680000 ± 17.595954, best_reward: 165.680000 ± 17.595954 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10001it [00:17, 557.84it/s, env_step=30000, len=147, loss=0.055, n/ep=0, n/st=10, rew=147.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 168.490000 ± 19.998747, best_reward: 168.490000 ± 19.998747 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10001it [00:16, 591.92it/s, env_step=40000, len=190, loss=0.019, n/ep=0, n/st=10, rew=190.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 144.990000 ± 7.681790, best_reward: 168.490000 ± 19.998747 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10001it [00:15, 631.93it/s, env_step=50000, len=160, loss=0.022, n/ep=0, n/st=10, rew=160.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 159.680000 ± 7.245523, best_reward: 168.490000 ± 19.998747 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 10001it [00:14, 700.10it/s, env_step=60000, len=155, loss=0.133, n/ep=0, n/st=10, rew=155.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 135.670000 ± 15.665283, best_reward: 168.490000 ± 19.998747 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7:   9%|8         | 880/10000 [00:02<00:28, 321.05it/s, env_step=60880, len=200, n/ep=1, n/st=10, rew=200.00]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training! Use 105.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, train_collector, test_collector, epoch, step_per_epoch, step_per_collect,\n",
    "    test_num, batch_size, update_per_step=1 / step_per_collect,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,\n",
    "    logger=logger)\n",
    "print(f'Finished training! Use {result[\"duration\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')\n",
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda3\\envs\\rl_env\\lib\\site-packages\\tianshou\\data\\collector.py:66: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n/ep': 1,\n",
       " 'n/st': 200,\n",
       " 'rews': array([200.]),\n",
       " 'lens': array([200]),\n",
       " 'idxs': array([0]),\n",
       " 'rew': 200.0,\n",
       " 'len': 200.0,\n",
       " 'rew_std': 0.0,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(eps_test)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b9c7ed270f474c0b985bf49c8749526c0e6b1f84c1b1c6765e44e2938a6f9c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}